#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
__author__ = "@YuweiYin"
"""

import os
import sys
import time
import logging
from typing import Optional

import fire

from datasets import Dataset
import openai
from openai import APIError, APIConnectionError, RateLimitError

from utils.init_functions import logger_setup, cuda_setup, random_setup
from utils.data_io import DataIO
from tasks.tasks_utils import *


class GPTGen:

    def __init__(
            self,
            verbose: bool,
            logger,
            cuda_dict: dict,
            seed: int = 42,
            cache_dir: Optional[str] = None,
            project_dir: Optional[str] = None,
            openai_model: str = "gpt-5-mini",
            openai_api_key: Optional[str] = None,
            bsz: int = 1,
            show_generation: bool = False,
            debug: bool = False,
            output_dir: Optional[str] = None,
            max_eval_num: int = -1,
            gen_temperature: float = 0.0,
            swi_version: int = 0,
            use_swi: bool = False,
            use_cot: bool = False,
            use_arr: bool = False,
            use_ps: bool = False,
    ):
        self.verbose = verbose
        self.logger = logger
        self.cuda_dict = cuda_dict
        self.seed = seed
        self.show_generation = show_generation  # If True, show outputs during generation
        self.debug = debug

        if isinstance(project_dir, str) and os.path.isdir(project_dir):
            self.project_dir = project_dir
        else:
            self.project_dir = os.getcwd()
        assert os.path.isdir(project_dir)

        self.output_dir = output_dir
        self.bsz = bsz
        self.max_eval_num = max_eval_num
        self.gen_temperature = gen_temperature
        self.swi_version = swi_version
        self.use_swi = use_swi
        self.use_cot = use_cot
        self.use_arr = use_arr
        self.use_ps = use_ps

        self.task_class_dict = TASK_CLASS_DICT
        self.sum_class_dict = SUM_CLASS_DICT
        self.qa_class_dict = QA_CLASS_DICT
        self.math_class_dict = MATH_CLASS_DICT

        # Cache directory
        self.home_dir = os.path.expanduser("~")
        if isinstance(cache_dir, str) and os.path.isdir(cache_dir):
            self.cache_dir = cache_dir
        else:
            self.cache_dir = os.path.join(self.home_dir, ".cache/huggingface")
            # self.cache_dir = os.path.join(self.project_dir, ".cache/huggingface/")
            if not os.path.isdir(self.cache_dir):
                os.makedirs(self.cache_dir, exist_ok=True)
        if self.verbose:
            self.logger.info(f">>> cache_dir: {self.cache_dir}")

        os.environ["HF_HOME"] = self.cache_dir

        # OpenAI settings
        # openai.organization = "YOUR_ORG_ID"
        if isinstance(openai_api_key, str) and len(openai_api_key) > 0:
            self.openai_api_key = openai_api_key
        else:
            self.openai_api_key = os.getenv("OPENAI_API_KEY")
        self.openai_model = openai_model

    @staticmethod
    def call_gpt(
            openai_model_name: str,
            messages,
            openai_api_key: Optional[str] = None,
            format_class=None,
            temperature: float = 1.0,
            api_call_sleep: int = 3,
            api_retry_limit: int = 10,
            api_retry_sleep: int = 10,
    ):
        """
        :param openai_model_name: The name of the OpenAI model to call.
        :param openai_api_key: A valid OpenAI API Key or from env var ${OPENAI_API_KEY}. https://platform.openai.com/
        :param messages: The input messages for the model.
        :param format_class: A BaseModel to define the output response format.
        :param temperature: The generation temperature that controls randomness.
        :param api_call_sleep: The sleep time between API calls (to avoid hitting the limit).
        :param api_retry_limit: The number of retries before giving up API calls.
        :param api_retry_sleep: The sleep time before retrying API calls.
        :return: The raw response generated by the model.
        """

        # Set up the input prompt (dialog-style) for GPT
        if isinstance(messages, list) and len(messages) > 0:
            input_messages = messages
        elif isinstance(messages, str) and len(messages) > 0:
            input_messages = [
                {"role": "developer", "content": "You are a helpful assistant."},
                {"role": "user", "content": messages},
            ]
        else:
            raise ValueError(f">>> Unsupported `messages`: {messages}")

        if openai_model_name.startswith("gpt-5"):
            # Note: GPT-5 models only accept the default temperature value: 1.0
            temperature = float(1.0)
        else:
            temperature = float(max(0.0, temperature))

        # assert api_call_sleep > 0 and api_retry_limit > 0 and api_retry_sleep > 0
        if not (isinstance(openai_api_key, str) and len(openai_api_key) > 0):
            openai_api_key = os.getenv("OPENAI_API_KEY")
        assert isinstance(openai_api_key, str) and len(openai_api_key) > 0, \
            f">>> AssertionError: openai_api_key: {openai_api_key}"

        try_cnt = 0
        while True:
            try_cnt += 1
            try:
                # OpenAI request
                if format_class is None:
                    response = openai.OpenAI(api_key=openai_api_key).with_options(
                        timeout=900.0).chat.completions.create(
                        model=openai_model_name,
                        messages=input_messages,
                        # messages=[
                        #     {"role": "developer", "content": system_prompt},
                        #     {"role": "user", "content": user_prompt},
                        # ],
                        temperature=temperature,
                        # response_format=None,
                        # service_tier="flex",
                    )
                else:
                    # response = openai.OpenAI(api_key=openai_api_key).with_options(timeout=900.0).beta.chat.completions.parse(
                    response = openai.OpenAI(api_key=openai_api_key).with_options(timeout=900.0).chat.completions.parse(
                        model=openai_model_name,
                        messages=input_messages,
                        # messages=[
                        #     {"role": "developer", "content": system_prompt},
                        #     {"role": "user", "content": user_prompt},
                        # ],
                        temperature=temperature,
                        response_format=format_class,
                        # service_tier="flex",
                    )
                if api_call_sleep > 0:  # Regular sleep time before running next data item
                    time.sleep(api_call_sleep)
                break
            except APIConnectionError as e:
                logging.info(f">>> APIConnectionError !!! >>> Failed to connect to OpenAI API: {e}")
                if try_cnt >= api_retry_limit:
                    sys.exit(1)
                time.sleep(api_retry_sleep)  # Sleep time before retrying API calls
            except APIError as e:
                logging.info(f">>> APIError !!! >>> OpenAI API Error: {e}")
                if try_cnt >= api_retry_limit:
                    sys.exit(1)
                time.sleep(api_retry_sleep)  # Sleep time before retrying API calls
            except RateLimitError as e:
                logging.info(f">>> RateLimitError !!! >>> OpenAI API request exceeded rate limit: {e}")
                if try_cnt >= api_retry_limit:
                    sys.exit(1)
                time.sleep(api_retry_sleep)  # Sleep time before retrying API calls
            except Exception as e:
                logging.info(f">>> Exception !!! >>> OpenAI Exception: {e}")
                if try_cnt >= api_retry_limit:
                    sys.exit(1)
                time.sleep(api_retry_sleep)  # Sleep time before retrying API calls

        return response

    def gpt_generate(
            self,
            eval_task_name: str,
    ):
        # Generation Phase: load datasets, load the model, set prompts, freely generation,
        #   and save results to JSON files (task/dataset information, input, and output)
        #   [optionally, apply chat templates for Chat models]

        os.environ["TOKENIZERS_PARALLELISM"] = "false"
        assert isinstance(self.output_dir, str), "Please specify --output_dir"
        assert eval_task_name in self.task_class_dict, \
            f"AssertionError: task name {eval_task_name} not in task_class_dict"
        eval_task_class = self.task_class_dict[eval_task_name]

        eval_task_obj = eval_task_class(
            verbose=self.verbose,
            logger=self.logger,
            cache_dir=self.cache_dir,
            project_dir=self.project_dir,
        )

        self.logger.info(f">>> Evaluation Task: {eval_task_name}")
        task_info = eval_task_obj.load_task()
        dataset_list = task_info["data"]

        # Deal with each task (and sub-tasks) - construct all input prompts
        all_prompts = dict()
        done_cnt = 0
        skip_cnt = 0
        for dataset_dict in dataset_list:
            cur_prompts = []

            ds_name, subset = dataset_dict["hf_dataset"], dataset_dict["hf_subset"]
            eval_split, eval_dataset = dataset_dict["eval_split"], dataset_dict["eval_dataset"]
            assert isinstance(eval_dataset, Dataset)
            len_dataset = len(eval_dataset)
            assert isinstance(ds_name, str) and len(ds_name) > 0
            if isinstance(subset, str) and len(subset) > 0:
                ds_id = f"{ds_name}---{subset}"
            else:
                ds_id = ds_name
            if self.verbose:
                self.logger.info(f">>> [Dataset: {ds_id}] [Eval: {eval_split}] # = {len_dataset}")

            if "options" in dataset_dict:
                ds_options = list(dataset_dict["options"])
            else:
                ds_options = []

            for idx, data_item in enumerate(eval_dataset):
                assert isinstance(data_item, dict)
                data_item["__ds_options"] = ds_options

                prompt_dict = eval_task_obj.set_dialog(
                    ds_name=ds_name,
                    subset=subset,
                    data_item=data_item,
                    use_swi=self.use_swi,
                    use_cot=self.use_cot,
                    use_arr=self.use_arr,
                    use_ps=self.use_ps,
                    swi_version=self.swi_version,
                )

                if not isinstance(prompt_dict, dict) or len(prompt_dict) == 0:
                    skip_cnt += 1
                else:
                    done_cnt += 1
                    cur_prompts.append(prompt_dict)
                    if len(cur_prompts) >= self.max_eval_num > 0:
                        break  # `max_eval_num` limits the number of instances (per subtask) to evalaute

            all_prompts[ds_id] = {
                "prompts": cur_prompts,
                "ds_info": {
                    "ds_id": ds_id,
                    "ds_name": ds_name,
                    "subset": subset,
                    "eval_split": eval_split,
                    # "eval_dataset": eval_dataset,
                    "ds_options": ds_options,
                },
            }
        self.logger.info(f">>> all_prompts: done_cnt = {done_cnt}; skip_cnt = {skip_cnt}")

        # Run generation for each item in each ds (subtask)
        all_results = dict()
        show_cnt = 100
        for ds_id, cur_prompts_dict in all_prompts.items():
            cur_results = []

            cur_prompts = list(cur_prompts_dict["prompts"])
            len_dataset = len(cur_prompts)
            ds_info = dict(cur_prompts_dict["ds_info"])
            ds_name, subset, eval_split = ds_info["ds_name"], ds_info["subset"], ds_info["eval_split"]

            self.logger.info(f">>> [Dataset: {ds_id}] [Eval: {eval_split}] # = {len_dataset}")

            # Split the input list into mini batches
            # assert isinstance(self.bsz, int) and self.bsz >= 1
            # prompt_batches = [cur_prompts[_i: _i + self.bsz] for _i in range(0, len(cur_prompts), self.bsz)]
            # num_batches = len(prompt_batches)

            # Note: Currently, we only use bsz=1 for GPT generation
            assert isinstance(self.bsz, int) and self.bsz == 1

            # Run generation
            for item_idx, cur_prompt_dict in enumerate(cur_prompts):
                assert isinstance(cur_prompt_dict, dict)

                # Obtain the system prompt and user prompt for GPT
                dialog = cur_prompt_dict["dialog"]
                assert isinstance(dialog, list) and len(dialog) == 2
                dialog_sys, dialog_user = dialog[0], dialog[1]
                assert isinstance(dialog_sys, dict) and "role" in dialog_sys and dialog_sys["role"] == "system"
                assert isinstance(dialog_user, dict) and "role" in dialog_user and dialog_user["role"] == "user"
                assert "content" in dialog_sys and "content" in dialog_user
                system_prompt = str(dialog_sys["content"]).strip()
                user_prompt = str(dialog_user["content"]).strip()
                gpt_input_messages = [
                    {"role": "developer", "content": system_prompt},
                    {"role": "user", "content": user_prompt},
                ]

                # Send OpenAI request
                response = self.call_gpt(
                    openai_model_name=self.openai_model, messages=gpt_input_messages,
                    openai_api_key=self.openai_api_key, temperature=self.gen_temperature,
                )  # format_class=format_class,
                res_message = response.choices[0].message

                # If the model refuses to respond, get the refusal message
                refusal = res_message.refusal
                if refusal:
                    self.logger.info(f">>> !!! >>> The model refuses to respond: {refusal}")

                cur_gen_output = {
                    "model": self.openai_model,
                    "ds_name": ds_name,
                    "subset": subset,
                    "eval_split": eval_split,
                    "ds_id": ds_id,
                    # "batch_idx": batch_idx,
                    "item_idx": item_idx,
                    # "prompt": str(gen_dict["prompt"]),  # The input prompt
                    # "input_text": str(gen_dict["input_text"]),  # The input text
                    "input_text": f"{system_prompt}\n\n{user_prompt}",  # The input text
                    "output_text": str(res_message.content).strip(),  # The LLM output (excluding the input)
                    # "len_input": int(gen_dict["len_input"]),  # Number of tokens of the model input/prompt
                    # "end_with_eot": bool(gen_dict["end_with_eot"]),  # True if the output ends with end-of-text
                    # "answers": cur_prompt_dict["answers"],  # The golden answers: List[str]
                    # "info": cur_prompt_dict["info"],
                    **cur_prompt_dict
                }

                cur_results.append(cur_gen_output)
                if self.verbose and len(cur_results) % show_cnt == 0:
                    self.logger.info(f">>> Progress: [{ds_id}] [Item: {len(cur_results)} / {len_dataset}]")
                    # DataIO.save_jsonl(output_fp, all_results, mode="w", verbose=False)

            all_results[ds_id] = cur_results

        # Save the generation outputs and show logs
        output_dir = os.path.join(self.output_dir, eval_task_name, self.openai_model)
        os.makedirs(output_dir, exist_ok=True)
        output_fp = os.path.join(output_dir, "results_gen.json")
        if os.path.exists(output_fp):
            self.logger.info(f"Results will be overwritten: {output_fp}")
        else:
            self.logger.info(f"Results will be saved at: {output_fp}")
        DataIO.save_json(output_fp, all_results, indent=2, verbose=self.verbose)
        self.logger.info(
            f">>> DONE ALL. openai_model = {self.openai_model};\n"
            f"use_swi: {self.use_swi}, gen_temperature: {self.gen_temperature}, batch_size: {self.bsz}"
        )


def main(
    task: int = 1,
    eval_task_name: Optional[str] = None,
    openai_model: str = "gpt-5-mini",
    openai_api_key: Optional[str] = None,
    cache_dir: Optional[str] = None,
    project_dir: Optional[str] = None,
    seed: int = 42,
    cuda: Optional[str] = None,
    bsz: int = 1,
    verbose: bool = False,
    debug: bool = False,
    output_dir: Optional[str] = None,
    max_eval_num: int = -1,
    gen_temperature: float = 0.0,
    swi_version: int = 0,
    use_swi: bool = False,
    use_cot: bool = False,
    use_arr: bool = False,
    use_ps: bool = False,
    **kwargs
) -> None:
    """
    [Stage 1: Reasoning & Answer Generation]
    Run LLM Generation on different tasks and benchmarks. Store the answers.
    (zero-shot generation w/ or w/o SWI: Speaking with Intent)

    :param task: 1. language model generation.
    :param eval_task_name: The name(s) of the evaluation task. (e.g., "xsum", "xlsum", and "gsm8k,math500")
    :param openai_model: The OpenAI model to generate results, e.g., "gpt-5-mini" and "gpt-4o"
    :param openai_api_key: The OpenAI API KEY. None: loading from the env variable ${OPENAI_API_KEY}.
    :param cache_dir: The root directory of the cache.
    :param project_dir: The root directory of the current project/repo.
    :param seed: Random seed of all modules.
    :param cuda: To specify CUDA GPU devices, e.g., "0" OR "0,1". Default: None -- Use CPU or all available GPUs.
    :param bsz: The batch size.
    :param verbose: Verbose mode: show logs.
    :param debug: Debugging / developing mode.
    :param output_dir: The path to the output file where the result metrics will be saved.
    :param max_eval_num: The maximum number of instance (per subtask) to evaluate.
    :param gen_temperature: The temperature used in LLM generation. Default: 0.0
    :param swi_version: The SWI prompt version to use. Default: 0. Choices: 0, 1, 2, 3, 4, 5.
    :param use_swi: Use our SWI method or not. SWI: Speaking with Intent
    :param use_cot: Use zero-shot Chain-of-Thought prompting method or not.  https://arxiv.org/abs/2205.11916
    :param use_arr: Use ARR method or not. ARR: Analyzing, Retrieving, and Reasoning.  https://arxiv.org/abs/2502.04689
    :param use_ps: Use Plan-and-Solve method or not.  https://aclanthology.org/2023.acl-long.147/
    :return: None.
    """

    timer_start = time.perf_counter()

    # Setup of the logger, CUDA gpus, and random seed
    logger = logger_setup("GPT_Gen")
    cuda_dict = cuda_setup(cuda=cuda, logger=logger, verbose=verbose)
    random_setup(seed=seed, has_cuda=cuda_dict["has_cuda"])

    if isinstance(kwargs, dict) and len(kwargs) > 0:
        logger.info(f">>> Extra parameters in kwargs: {kwargs}")

    if isinstance(cache_dir, str) and os.path.isdir(cache_dir):
        os.environ["HF_HOME"] = cache_dir
        # os.environ["HF_HOME"] = os.path.join(cache_dir, "datasets")
        # os.environ["HF_HOME"] = os.path.join(cache_dir, "hub")
    else:
        cache_dir = None

    from transformers import modeling_utils

    if not hasattr(modeling_utils, "ALL_PARALLEL_STYLES") or modeling_utils.ALL_PARALLEL_STYLES is None:
        logger.info(f">>> Manually fix potential issues about `modeling_utils.ALL_PARALLEL_STYLES`")
        modeling_utils.ALL_PARALLEL_STYLES = ["tp", "none", "colwise", "rowwise"]

    gpt_gen = GPTGen(
        verbose=verbose,
        logger=logger,
        cuda_dict=cuda_dict,
        seed=seed,
        cache_dir=cache_dir,
        project_dir=project_dir,
        openai_model=openai_model,
        openai_api_key=openai_api_key,
        bsz=max(int(bsz), 1),
        debug=debug,
        output_dir=output_dir,
        max_eval_num=int(max_eval_num),
        gen_temperature=float(gen_temperature),
        swi_version=int(swi_version),
        use_swi=use_swi,
        use_cot=use_cot,
        use_arr=use_arr,
        use_ps=use_ps,
    )

    task = int(task)
    match task:
        case 1:
            if isinstance(eval_task_name, tuple) or isinstance(eval_task_name, list):
                for cur_task_name in eval_task_name:
                    cur_task_name = str(cur_task_name).strip()
                    logger.info(f">>> <START> {cur_task_name}\n")
                    gpt_gen.gpt_generate(eval_task_name=cur_task_name)
                    logger.info(f">>> <END> {cur_task_name}\n\n\n")
            elif isinstance(eval_task_name, str):
                eval_task_name = str(eval_task_name).strip()
                logger.info(f">>> <START> {eval_task_name}\n")
                gpt_gen.gpt_generate(eval_task_name=eval_task_name)
                logger.info(f">>> <END> {eval_task_name}\n\n\n")
            else:
                raise ValueError(f"--eval_task_name should be a tuple/list/str: {eval_task_name}")
        case _:
            raise ValueError(f"ValueError: task = {task}")

    timer_end = time.perf_counter()
    total_sec = timer_end - timer_start
    logger.info(f"Total Running Time: {total_sec:.1f} sec ({total_sec / 60:.1f} min; {total_sec / 3600:.2f} h)")


if __name__ == "__main__":
    fire.Fire(main)
